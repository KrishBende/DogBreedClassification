{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e0c079e-4cb6-42c1-a837-25b097bc2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b4713-3512-486b-a57e-c43b4150c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_directories(base_path, species_list):\n",
    "    splits = ['train', 'val', 'test']\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(base_path, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        for species in species_list:\n",
    "            os.makedirs(os.path.join(split_path, species), exist_ok=True)\n",
    "\n",
    "def split_dataset(src_dir, dest_dir, train_ratio=0.7, val_ratio=0.15):\n",
    "    species_list = [d for d in os.listdir(src_dir) if os.path.isdir(os.path.join(src_dir, d))]\n",
    "    \n",
    "    create_split_directories(dest_dir, species_list)\n",
    "    \n",
    "    for species in species_list:\n",
    "        species_dir = os.path.join(src_dir, species)\n",
    "        files = os.listdir(species_dir)\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        total = len(files)\n",
    "        train_end = int(total * train_ratio)\n",
    "        val_end = train_end + int(total * val_ratio)\n",
    "        \n",
    "        train_files = files[:train_end]\n",
    "        val_files = files[train_end:val_end]\n",
    "        test_files = files[val_end:]\n",
    "        \n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(species_dir, file), os.path.join(dest_dir, 'train', species, file))\n",
    "        for file in val_files:\n",
    "            shutil.copy(os.path.join(species_dir, file), os.path.join(dest_dir, 'val', species, file))\n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(species_dir, file), os.path.join(dest_dir, 'test', species, file))\n",
    "\n",
    "src_dir = 'dataset'  # Path to original dataset directory\n",
    "dest_dir = 'SplitDataset'  # Path to save the split dataset\n",
    "\n",
    "split_dataset(src_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a9ddbc-c727-4aae-9471-295abe73b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA LOADING AND TRANSFORMATION\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'SplitDataset'     #ADD DATASET LOCATION FOR TRAINING, with seperate 'train' 'val' and 'test' folders\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val','test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val','test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ef6239-bb81-435c-9b3b-e08b8694a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\plantclassification\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\krish\\plantclassification\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#MODEL INITIALISATION\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8078ddce-c1eb-44b4-8a9c-fff3c9befd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.3693 Acc: 0.1704\n",
      "val Loss: 2.0453 Acc: 0.2587\n",
      "Checkpoint saved at epoch 0\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.9379 Acc: 0.3793\n",
      "val Loss: 1.4729 Acc: 0.7273\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.5055 Acc: 0.6874\n",
      "val Loss: 1.0103 Acc: 0.8392\n",
      "Checkpoint saved at epoch 2\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.1869 Acc: 0.7689\n",
      "val Loss: 0.6990 Acc: 0.9161\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.9713 Acc: 0.8207\n",
      "val Loss: 0.5215 Acc: 0.8881\n",
      "Checkpoint saved at epoch 4\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.7714 Acc: 0.8607\n",
      "val Loss: 0.4222 Acc: 0.9441\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.6729 Acc: 0.8756\n",
      "val Loss: 0.3369 Acc: 0.9371\n",
      "Checkpoint saved at epoch 6\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.6080 Acc: 0.8859\n",
      "val Loss: 0.3362 Acc: 0.9371\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.6293 Acc: 0.8711\n",
      "val Loss: 0.3312 Acc: 0.9301\n",
      "Checkpoint saved at epoch 8\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.6211 Acc: 0.8800\n",
      "val Loss: 0.3224 Acc: 0.9301\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.6142 Acc: 0.8681\n",
      "val Loss: 0.3205 Acc: 0.9301\n",
      "Checkpoint saved at epoch 10\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.5731 Acc: 0.8844\n",
      "val Loss: 0.3130 Acc: 0.9371\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.6080 Acc: 0.8859\n",
      "val Loss: 0.3074 Acc: 0.9371\n",
      "Checkpoint saved at epoch 12\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.6182 Acc: 0.8652\n",
      "val Loss: 0.3015 Acc: 0.9580\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.5646 Acc: 0.8933\n",
      "val Loss: 0.3044 Acc: 0.9371\n",
      "Checkpoint saved at epoch 14\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.5917 Acc: 0.8770\n",
      "val Loss: 0.2999 Acc: 0.9580\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.5713 Acc: 0.8785\n",
      "val Loss: 0.3012 Acc: 0.9580\n",
      "Checkpoint saved at epoch 16\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.5755 Acc: 0.8889\n",
      "val Loss: 0.3015 Acc: 0.9580\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.5740 Acc: 0.8756\n",
      "val Loss: 0.2990 Acc: 0.9371\n",
      "Checkpoint saved at epoch 18\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.5692 Acc: 0.8919\n",
      "val Loss: 0.2988 Acc: 0.9371\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.5538 Acc: 0.8933\n",
      "val Loss: 0.3020 Acc: 0.9371\n",
      "Checkpoint saved at epoch 20\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.8756\n",
      "val Loss: 0.3013 Acc: 0.9371\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.5621 Acc: 0.8889\n",
      "val Loss: 0.2984 Acc: 0.9371\n",
      "Checkpoint saved at epoch 22\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.6023 Acc: 0.8563\n",
      "val Loss: 0.2968 Acc: 0.9580\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.5450 Acc: 0.9022\n",
      "val Loss: 0.2978 Acc: 0.9580\n",
      "Checkpoint saved at epoch 24\n",
      "\n",
      "Training complete in 5m 14s\n",
      "Best val Acc: 0.9580\n"
     ]
    }
   ],
   "source": [
    "#MODEL TRAINING\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, save_path='modeltrain.pth', checkpoint_interval=2):\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "\n",
    "            if epoch % checkpoint_interval == 0:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f'Checkpoint saved at epoch {epoch}')\n",
    "\n",
    "            print()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted. Saving the model...\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'Model saved to {save_path}')\n",
    "        return model\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    return model\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25, save_path='modeltrain.pth', checkpoint_interval=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4129b0fe-f3a9-417e-a261-a0ef3ecdd1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_15484\\3159691593.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_ft.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3213 Acc: 0.9664\n"
     ]
    }
   ],
   "source": [
    "#TEST LOSS AND ACCURACY\n",
    "\n",
    "save_path='modeltrain.pth'\n",
    "model_ft.load_state_dict(torch.load(save_path))\n",
    "model_ft = model_ft.to(device)\n",
    "def evaluate_model_with_predictions(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    total_loss = running_loss / dataset_sizes['test']\n",
    "    total_acc = running_corrects.double() / dataset_sizes['test']\n",
    "\n",
    "    print(f'Test Loss: {total_loss:.4f} Acc: {total_acc:.4f}')\n",
    "    return total_loss, total_acc, all_preds, all_labels\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate_model_with_predictions(model_ft, dataloaders['test'], criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53fc3f25-2320-4275-87cc-e81b5194a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_15484\\530840440.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_ft.load_state_dict(torch.load('modeltrain.pth'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m img_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     23\u001b[0m img_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img_rgb)\n\u001b[1;32m---> 24\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_pil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m img_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m img_tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\plantclassification\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\plantclassification\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\plantclassification\\Lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft.load_state_dict(torch.load('modeltrain.pth'))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    img_tensor = preprocess(img_pil)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ft(img_tensor)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        pred_class = class_names[preds[0]]\n",
    "\n",
    "    cv2.putText(frame, f'Prediction: {pred_class}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
